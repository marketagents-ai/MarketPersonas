{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan umap-learn scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPORTS & LOADING\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# HDBSCAN & UMAP\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "def load_embeddings_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all .npy files from the given folder\n",
    "    and returns a single numpy array of embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            embeddings = np.load(file_path)\n",
    "            # If each file has shape (D,) or (1, D), adjust as needed.\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    # Stack into a single array (N, D)\n",
    "    # If each file is shape (D,), do np.vstack(all_embeddings).\n",
    "    # If shape (N, D), adjust accordingly.\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Set your folder path here\n",
    "folder_path = \"\"\n",
    "X = load_embeddings_from_folder(folder_path)\n",
    "print(\"Embeddings shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  K-MEANS\n",
    "def kmeans_clustering(X, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Performs K-Means clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return labels, kmeans\n",
    "\n",
    "labels_kmeans, model_kmeans = kmeans_clustering(X, n_clusters=5)\n",
    "print(\"K-Means labels:\", np.unique(labels_kmeans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGLOMERATIVE (HIERARCHICAL) CLUSTERING\n",
    "def hierarchical_clustering(X, n_clusters=5, linkage='ward'):\n",
    "    \"\"\"\n",
    "    Performs Agglomerative (Hierarchical) Clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    agg = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        affinity='euclidean',\n",
    "        linkage=linkage\n",
    "    )\n",
    "    labels = agg.fit_predict(X)\n",
    "    return labels, agg\n",
    "\n",
    "labels_hier, model_hier = hierarchical_clustering(X, n_clusters=5, linkage='ward')\n",
    "print(\"Hierarchical labels:\", np.unique(labels_hier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "def dbscan_clustering(X, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Performs DBSCAN clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    labels = dbscan_model.fit_predict(X)\n",
    "    return labels, dbscan_model\n",
    "\n",
    "labels_dbscan, model_dbscan = dbscan_clustering(X, eps=0.5, min_samples=5)\n",
    "print(\"DBSCAN labels:\", np.unique(labels_dbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "def hdbscan_clustering(X, min_cluster_size=5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Performs HDBSCAN clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    labels = hdbscan_model.fit_predict(X)\n",
    "    return labels, hdbscan_model\n",
    "\n",
    "labels_hdbscan, model_hdbscan = hdbscan_clustering(X, min_cluster_size=5, min_samples=5)\n",
    "print(\"HDBSCAN labels:\", np.unique(labels_hdbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  UMAP + HDBSCAN\n",
    "def umap_hdbscan_clustering(X, n_neighbors=15, n_components=5, min_cluster_size=5, min_samples=5):\n",
    "    \"\"\"\n",
    "    First reduces embeddings to n_components dimensions using UMAP,\n",
    "    then performs HDBSCAN on the reduced data.\n",
    "    Returns labels and a tuple (umap_reducer, hdbscan_model).\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    \n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    labels = hdbscan_model.fit_predict(X_reduced)\n",
    "    return labels, (reducer, hdbscan_model)\n",
    "\n",
    "labels_umap_hdbscan, (umap_model, hdb_model) = umap_hdbscan_clustering(X)\n",
    "print(\"UMAP+HDBSCAN labels:\", np.unique(labels_umap_hdbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUPYTER NOTEBOOK CELL 8: EVALUATION (SILHOUETTE SCORE)\n",
    "def evaluate_clustering(X, labels, cluster_name=\"Cluster\"):\n",
    "    \"\"\"\n",
    "    Evaluates clustering with Silhouette Score if there are more than 1 valid cluster.\n",
    "    (Excluding -1 if using DBSCAN/HDBSCAN)\n",
    "    \"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    # Remove noise label (-1) from the count if present\n",
    "    unique_labels_no_noise = unique_labels - {-1}\n",
    "    \n",
    "    # Need at least 2 valid clusters\n",
    "    if len(unique_labels_no_noise) > 1:\n",
    "        score = silhouette_score(X, labels)\n",
    "        print(f\"{cluster_name} Silhouette Score: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{cluster_name} has too few clusters or mostly noise. Cannot compute Silhouette Score.\")\n",
    "\n",
    "# Evaluate each clustering result\n",
    "evaluate_clustering(X, labels_kmeans, \"K-Means\")\n",
    "evaluate_clustering(X, labels_hier, \"Hierarchical\")\n",
    "evaluate_clustering(X, labels_dbscan, \"DBSCAN\")\n",
    "evaluate_clustering(X, labels_hdbscan, \"HDBSCAN\")\n",
    "\n",
    "X_reduced = umap_model.transform(X)\n",
    "evaluate_clustering(X_reduced, labels_umap_hdbscan, \"UMAP+HDBSCAN\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

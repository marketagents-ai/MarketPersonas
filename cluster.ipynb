{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdbscan umap-learn scikit-learn numpy sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# For embedding: Sentence Transformers (pip install sentence-transformers)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def embed_description(text, model):\n",
    "    \"\"\"\n",
    "    Takes a text string and a SentenceTransformer model,\n",
    "    returns the embedding as a numpy array.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return np.array([])\n",
    "    return model.encode(text)\n",
    "\n",
    "def embed_schema_folder(folder_path, model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Iterates over all .json files in a folder. For each JSON schema:\n",
    "      - Embeds the 'description' at the top level (if present).\n",
    "      - Embeds the 'description' of each property (if present).\n",
    "      - Saves the resulting embeddings as .npy files in the same folder.\n",
    "    \"\"\"\n",
    "    # Load the SentenceTransformer model once\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                try:\n",
    "                    schema = json.load(f)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Could not parse JSON from {file_name}. Skipping...\")\n",
    "                    continue\n",
    "\n",
    "            # 1) Embed the main schema 'description' (if it exists)\n",
    "            main_description = schema.get('description', '')\n",
    "            main_embedding = embed_description(main_description, model)\n",
    "            if main_embedding.size > 0:\n",
    "                # Save the main description embedding\n",
    "                np.save(\n",
    "                    os.path.join(folder_path, f\"{file_name}_main_desc_embedding.npy\"),\n",
    "                    main_embedding\n",
    "                )\n",
    "\n",
    "            # 2) Embed each property's description (if properties exist)\n",
    "            props = schema.get('properties', {})\n",
    "            for prop_name, prop_info in props.items():\n",
    "                prop_description = prop_info.get('description', '')\n",
    "                prop_embedding = embed_description(prop_description, model)\n",
    "                if prop_embedding.size > 0:\n",
    "                    # Save the property description embedding\n",
    "                    np.save(\n",
    "                        os.path.join(folder_path, f\"{file_name}_{prop_name}_embedding.npy\"),\n",
    "                        prop_embedding\n",
    "                    )\n",
    "\n",
    "            print(f\"Processed schema file: {file_name}\")\n",
    "\n",
    "# Example usage:\n",
    "# folder_path = \"/path/to/your/schema/folder\"\n",
    "# embed_schema_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPORTS & LOADING\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# HDBSCAN & UMAP\n",
    "import hdbscan\n",
    "import umap\n",
    "\n",
    "def load_embeddings_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Loads all .npy files from the given folder\n",
    "    and returns a single numpy array of embeddings.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            embeddings = np.load(file_path)\n",
    "            # If each file has shape (D,) or (1, D), adjust as needed.\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    # Stack into a single array (N, D)\n",
    "    # If each file is shape (D,), do np.vstack(all_embeddings).\n",
    "    # If shape (N, D), adjust accordingly.\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "# Set your folder path here\n",
    "folder_path = \"\"\n",
    "X = load_embeddings_from_folder(folder_path)\n",
    "print(\"Embeddings shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  K-MEANS\n",
    "def kmeans_clustering(X, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Performs K-Means clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        n_init=10,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    )\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    return labels, kmeans\n",
    "\n",
    "labels_kmeans, model_kmeans = kmeans_clustering(X, n_clusters=5)\n",
    "print(\"K-Means labels:\", np.unique(labels_kmeans))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGLOMERATIVE (HIERARCHICAL) CLUSTERING\n",
    "def hierarchical_clustering(X, n_clusters=5, linkage='ward'):\n",
    "    \"\"\"\n",
    "    Performs Agglomerative (Hierarchical) Clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    agg = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters,\n",
    "        affinity='euclidean',\n",
    "        linkage=linkage\n",
    "    )\n",
    "    labels = agg.fit_predict(X)\n",
    "    return labels, agg\n",
    "\n",
    "labels_hier, model_hier = hierarchical_clustering(X, n_clusters=5, linkage='ward')\n",
    "print(\"Hierarchical labels:\", np.unique(labels_hier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "def dbscan_clustering(X, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Performs DBSCAN clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    dbscan_model = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "    labels = dbscan_model.fit_predict(X)\n",
    "    return labels, dbscan_model\n",
    "\n",
    "labels_dbscan, model_dbscan = dbscan_clustering(X, eps=0.5, min_samples=5)\n",
    "print(\"DBSCAN labels:\", np.unique(labels_dbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "def hdbscan_clustering(X, min_cluster_size=5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Performs HDBSCAN clustering.\n",
    "    Returns cluster labels and the trained model.\n",
    "    \"\"\"\n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    labels = hdbscan_model.fit_predict(X)\n",
    "    return labels, hdbscan_model\n",
    "\n",
    "labels_hdbscan, model_hdbscan = hdbscan_clustering(X, min_cluster_size=5, min_samples=5)\n",
    "print(\"HDBSCAN labels:\", np.unique(labels_hdbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  UMAP + HDBSCAN\n",
    "def umap_hdbscan_clustering(X, n_neighbors=15, n_components=5, min_cluster_size=5, min_samples=5):\n",
    "    \"\"\"\n",
    "    First reduces embeddings to n_components dimensions using UMAP,\n",
    "    then performs HDBSCAN on the reduced data.\n",
    "    Returns labels and a tuple (umap_reducer, hdbscan_model).\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        random_state=42\n",
    "    )\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    \n",
    "    hdbscan_model = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    labels = hdbscan_model.fit_predict(X_reduced)\n",
    "    return labels, (reducer, hdbscan_model)\n",
    "\n",
    "labels_umap_hdbscan, (umap_model, hdb_model) = umap_hdbscan_clustering(X)\n",
    "print(\"UMAP+HDBSCAN labels:\", np.unique(labels_umap_hdbscan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JUPYTER NOTEBOOK CELL 8: EVALUATION (SILHOUETTE SCORE)\n",
    "def evaluate_clustering(X, labels, cluster_name=\"Cluster\"):\n",
    "    \"\"\"\n",
    "    Evaluates clustering with Silhouette Score if there are more than 1 valid cluster.\n",
    "    (Excluding -1 if using DBSCAN/HDBSCAN)\n",
    "    \"\"\"\n",
    "    unique_labels = set(labels)\n",
    "    # Remove noise label (-1) from the count if present\n",
    "    unique_labels_no_noise = unique_labels - {-1}\n",
    "    \n",
    "    # Need at least 2 valid clusters\n",
    "    if len(unique_labels_no_noise) > 1:\n",
    "        score = silhouette_score(X, labels)\n",
    "        print(f\"{cluster_name} Silhouette Score: {score:.4f}\")\n",
    "    else:\n",
    "        print(f\"{cluster_name} has too few clusters or mostly noise. Cannot compute Silhouette Score.\")\n",
    "\n",
    "# Evaluate each clustering result\n",
    "evaluate_clustering(X, labels_kmeans, \"K-Means\")\n",
    "evaluate_clustering(X, labels_hier, \"Hierarchical\")\n",
    "evaluate_clustering(X, labels_dbscan, \"DBSCAN\")\n",
    "evaluate_clustering(X, labels_hdbscan, \"HDBSCAN\")\n",
    "\n",
    "X_reduced = umap_model.transform(X)\n",
    "evaluate_clustering(X_reduced, labels_umap_hdbscan, \"UMAP+HDBSCAN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "market",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
